{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "- Instead of using single function like sigmoid or linear to calculate. Neural Network provides a large amount of function like that. And it will be divided into multiple step called layers.\n",
    "\n",
    "- Each layer have nodes. At each nodes. We will calculate a value which is similar as linear regression or logistic regression\n",
    "\n",
    "- Previous layer's output will be the next layer's input. \n",
    "\n",
    "![Image](./image/NeuralNetwork.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "- For each node we need to pass our $f(w, b)$ through the activation function like: $sigmoid, tanh, Relu,$ ...\n",
    "\n",
    "![Image](./image/ActivationFunction.png)\n",
    "\n",
    "## Why we need non-linear Activation Function\n",
    "\n",
    "- **Non-linear Activation Is Essential**: The incorporation of non-linear activation functions is crucial for neural networks to realize their full potential in modeling complex, non-linear relationships within data.\n",
    "\n",
    "- **Special Cases for Linear Activation**: While linear activation functions are generally not used in hidden layers, they might be suitable for the output layer in regression problems, albeit with careful consideration of the specific task requirements, such as ensuring non-negative outputs where applicable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "- **Purpose**: Initiates the process by passing input data through the network to make predictions based on current weights and biases.\n",
    "\n",
    "- **Process**: Transforms data at each layer via weighted sums and activation functions, culminating in a prediction at the output layer.\n",
    "\n",
    "![Image](./image/Forward.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "\n",
    "- **Purpose**: Enables the network to learn from the error computed during Forward Propagation by adjusting its weights and biases. (This is based on Chain Rule)\n",
    "\n",
    "- **Process**: Calculates gradients of the loss function with respect to each network parameter in reverse, from output to input, using the chain rule.\n",
    "\n",
    "![Image](./image/CalDerivative.png)\n",
    "\n",
    "![Image](./image/BackPropagate.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Both Are Necessary\n",
    "- **Forward Propagation**: is used to making predictions and computing error\n",
    "- **Backward Propagation**: use this error to learn and adjust the model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
