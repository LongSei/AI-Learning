{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis for Cat Classification Model\n",
    "\n",
    "## Overview\n",
    "Error analysis is a critical step in improving the performance of our cat classification model. By examining the errors made by our algorithm, we can gain valuable insights into what aspects need refinement.\n",
    "\n",
    "\n",
    "## Example\n",
    "   ### Current Model Performance\n",
    "   - The model currently has a 10% error rate on the development set.\n",
    "\n",
    "   ### Discovery\n",
    "   - A review of mislabeled images revealed that some errors are due to the model confusing dogs with cats.\n",
    "\n",
    "   ### Proposed Error Analysis Approach\n",
    "\n",
    "   1. **Sample Analysis**\n",
    "      - Randomly select 100 mislabeled images from the development set.\n",
    "      - Manually count how many of these are dogs.\n",
    "\n",
    "   2. **Decision Criteria**\n",
    "      - If a significant portion (e.g., 50%) of the errors are due to dog images being misclassified as cats, prioritizing improvements in this area could substantially reduce the error rate.\n",
    "\n",
    "   3. **Outcome**\n",
    "      - If only a small fraction (e.g., 5 out of 100) are dog images, this may not be the most effective use of resources.\n",
    "      - If a large fraction (e.g., 50 out of 100) are dog images, focusing on distinguishing dogs from cats could reduce the error rate by up to half (the ceiling effect).\n",
    "\n",
    "   4. **Next Steps**\n",
    "      - Based on the findings from the error analysis, develop a plan to enhance the model's ability to differentiate between cats and dogs.\n",
    "      - Implement the improvements and re-evaluate the model's performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up Incorrectly Labeled Data\n",
    "\n",
    "- Deep learning algorithms are robust to random errors in the training set but less so to systematic errors. However, if possible, you should correct these labels.\n",
    "\n",
    "- If you want to check if the labels in the dev/test set are incorrectly assigned, you should also check the tally with the incorrect label column. For example:\n",
    "\n",
    "### Error Analysis Table\n",
    "\n",
    "| Image | Dog | Great Cat | Blurry | Incorrectly labeled | Comments |\n",
    "|-------|-----|-----------|--------|---------------------|----------|\n",
    "| ...   |     |           |        |                     |          |\n",
    "| 98    |     | ✓         |        |                     | Labeler missed cat in background |\n",
    "| 99    |     |           | ✓      |                     |          |\n",
    "| 100   |     |           |        | ✓                   | Drawing of a cat; Not a real cat. |\n",
    "| % of total | 8% | 43% | 61% | 6% | |\n",
    "\n",
    "\n",
    "- Overall dev set error: 10%\n",
    "\n",
    "- Errors due to incorrect labels: 0.6%\n",
    "\n",
    "- Errors due to other causes: 9.4%\n",
    "\n",
    "The goal of the dev set is to help you select between two classifiers A & B.\n",
    "\n",
    "\n",
    "- If the total error in the development set is 10%:\n",
    "  \n",
    "  - The error due to incorrect data is: 0.6%\n",
    "  \n",
    "  - The error due to other causes is: 9.4%\n",
    "\n",
    "- You should focus on the 9.4% rather than the incorrectly labeled data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your first system quickly, then iterate\n",
    "\n",
    "\n",
    "- The steps you take to make your deep learning project:\n",
    "  \n",
    "  - Setup dev/test set and metric\n",
    "  \n",
    "  - Build initial system quickly\n",
    "  \n",
    "  - Use Bias/Variance analysis & Error analysis to prioritize next steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing on different distributions\n",
    "\n",
    "\n",
    "- A lot of teams are working with deep learning applications that have training sets that are different from the dev/test sets due to the hunger of deep learning to data.\n",
    "\n",
    "- There are some strategies to follow up when training set distribution differs from dev/test sets distribution.\n",
    "  \n",
    "  - Option one (not recommended): shuffle all the data together and extract randomly training and dev/test sets.\n",
    "    \n",
    "    - Advantages: all the sets now come from the same distribution.\n",
    "    \n",
    "    - Disadvantages: the other (real world) distribution that was in the dev/test sets will occur less in the new dev/test sets and that might be not what you want to achieve.\n",
    "  \n",
    "  - Option two: take some of the dev/test set examples and add them to the training set.\n",
    "    \n",
    "    - Advantages: the distribution you care about is your target now.\n",
    "    \n",
    "    - Disadvantage: the distributions in training and dev/test sets are now different. But you will get a better performance over a long time.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance with mismatched data distributions\n",
    "\n",
    "\n",
    "- Bias and Variance analysis changes when training and Dev/test set is from a different distribution.\n",
    "\n",
    "\n",
    "- Example: the cat classification example. Suppose you've worked in the example and reached this:\n",
    "  \n",
    "  - Human error: 0%\n",
    "  \n",
    "  - Train error: 1%\n",
    "  \n",
    "  - Dev error: 10%\n",
    "\n",
    "- In this example, you'll think that this is a variance problem, but because the distributions aren't the same you can't tell for sure. Because it could be that train set was easy to train on, but the dev set was more difficult.\n",
    "\n",
    "\n",
    "- To solve this issue we create a new set called train-dev set as a random subset of the training set (so it has the same distribution) and we get:\n",
    "  \n",
    "  - Human error: 0%\n",
    "  \n",
    "  - Train error: 1%\n",
    "  \n",
    "  - Train-dev error: 9%\n",
    "  \n",
    "  - Dev error: 10%\n",
    "    \n",
    "    - Now we are sure that this is a high variance problem.\n",
    "\n",
    "\n",
    "- Suppose we have a different situation:\n",
    "  \n",
    "  - Human error: 0%\n",
    "  \n",
    "  - Train error: 1%\n",
    "  \n",
    "  - Train-dev error: 1.5%\n",
    "  \n",
    "  - Dev error: 10%\n",
    "    \n",
    "    - In this case we have something called Data mismatch problem.\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "1. Human-level error (proxy for Bayes error)\n",
    "\n",
    "2. Train error\n",
    "   - Calculate avoidable $bias\\space =\\space training\\space error\\space -\\space human\\space level\\space error$\n",
    "     - If the difference is big then its Avoidable bias problem then you should use a strategy for high bias.\n",
    "\n",
    "3. Train-dev error\n",
    "   - Calculate $variance\\space =\\space training-dev\\space error\\space -\\space training\\space error$\n",
    "     - If the difference is big then its high variance problem then you should use a strategy for solving it.\n",
    "\n",
    "4. Dev error\n",
    "   - Calculate $data\\space  mismatch\\space  =\\space  dev\\space  error\\space  -\\space  train-dev\\space  error$\n",
    "     - If difference is much bigger then train-dev error its Data mismatch problem.\n",
    "\n",
    "5. Test error\n",
    "   - Calculate degree of overfitting to $dev\\space set\\space =\\space test\\space error\\space -\\space dev\\space error$\n",
    "     - If the difference is big (positive) then maybe you need to find a bigger dev set (dev and test set come from the same distribution, so the only way for there to be a huge gap here, for it to do much better on the dev set than the test set, is if you somehow managed to overfit the dev set).\n",
    "\n",
    "Unfortunately, there aren't many systematic ways to deal with data mismatch. There are some things to try about this in the next section.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing data mismatch\n",
    "\n",
    "- There aren't completely systematic solutions to this, but there are some things you could try.\n",
    "\n",
    "1. Carry out manual error analysis to try to understand the difference between training and dev/test sets.\n",
    "2. Make training data more similar, or collect more data similar to dev/test sets.\n",
    "\n",
    "\n",
    "- If your goal is to make the training data more similar to your dev set one of the techniques you can use is $Artificial\\space data\\space synthesis$ that can help you make more training data.\n",
    "  \n",
    "  - Combine some of your training data with something that can convert it to the dev/test set distribution.\n",
    "    \n",
    "    - Examples:\n",
    "      \n",
    "      a. Combine normal audio with car noise to get audio with car noise example.\n",
    "      \n",
    "      b. Generate cars using 3D graphics in a car classification example.\n",
    "  \n",
    "  - Be cautious and bear in mind whether or not you might be accidentally simulating data only from a tiny subset of the space of all possible examples because your NN might overfit these generated data (like particular car noise or a particular design of 3D graphics cars).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Learning\n",
    "\n",
    "## Definition\n",
    "\n",
    "- Involves training a neural network to handle multiple tasks at once.\n",
    "\n",
    "- Tasks mutually enhance learning by sharing features.\n",
    "\n",
    "- Appropriate when sufficient data is available for each task and when features are universally applicable.\n",
    "\n",
    "\n",
    "## Comparison of Transfer and Multitask Learning\n",
    "\n",
    "- Transfer learning is more commonly favored in practice.\n",
    "\n",
    "- Multitask learning, while not as widespread as transfer learning, remains vital and effective, especially with large-scale network training.\n",
    "\n",
    "\n",
    "![Image](./image/Multi-Tasking.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
