{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Projects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonalization\n",
    "\n",
    "- To achieve good performance, it is necessary to **tune** the correct \"knobs\" of the machine learning system:\n",
    "  - Ensure good performance on the training set.\n",
    "\n",
    "  - Then, good performance on the development set (dev set).\n",
    "\n",
    "  - Next is good performance on the test set.\n",
    "  \n",
    "  - Finally, achieving good real-world application performance, leading to user satisfaction.\n",
    "\n",
    "- **Orthogonalization** helps identify and adjust specific \"knobs\" for each objective:\n",
    "  \n",
    "  - If the algorithm does not perform well on the training set, it may be necessary to increase the network size or use a different optimization algorithm like Adam.\n",
    "  \n",
    "  - If the performance on the dev set is not good, techniques such as **regularization** may need to be applied to improve generalization.\n",
    "  \n",
    "  - If the algorithm performs well on the dev set but not on the test set, considering expanding the dev set to avoid over-optimizing for the dev set is necessary.\n",
    "\n",
    "![Image](./image/Orthogonalization.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Number Evaluation Metric\n",
    "\n",
    "- Confusion matrix:\n",
    "\n",
    "  |               | Predicted cat | Predicted non-cat |\n",
    "  |---------------|---------------|-------------------|\n",
    "  | Actual cat    | 3             | 2                 |\n",
    "  | Actual non-cat| 1             | 4                 |\n",
    "\n",
    "- **Precision**: percentage of true cats in the recognized result: P = 3/(3 + 1)\n",
    "\n",
    "- **Recall**: percentage of true recognition cat of the all cat predictions: R = 3/(3 + 2)\n",
    "  \n",
    "- **Accuracy**: (3+4)/10\n",
    "\n",
    "  | Classifier | Precision | Recall |\n",
    "  |------------|-----------|--------|\n",
    "  | A          | 95%       | 90%    |\n",
    "  | B          | 98%       | 85%    |\n",
    "\n",
    "- **F1-Score** F1 = $\\frac{2}{(1/P) + (1/R)}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/dev/test distributions\n",
    "\n",
    "- Dev and test sets have to come from the same distribution.\n",
    "\n",
    "- Choose dev set and test set to reflect data you expect to get in the future and consider important to do well on.\n",
    "\n",
    "- Setting up the dev set, as well as the validation metric is really defining what target you want to aim at."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the dev and test sets\n",
    "\n",
    "  - An old way of splitting the data was 70% training, 30% test or 60% training, 20% dev, 20% test (Valid for a number of examples ~ <100000)\n",
    "    \n",
    "  - In the modern deep learning if you have a million or more examples a reasonable split would be 98% training, 1% dev, 1% test.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to change dev/test sets and metrics\n",
    "\n",
    "### Issue:\n",
    "\n",
    "- Sometimes, the evaluation metric (like classification error) does not reflect the actual priorities or objectives of the project.\n",
    "\n",
    "- An algorithm (A) might have a low error rate but allows undesirable images (such as pornographic content) to pass through.\n",
    "\n",
    "- Another algorithm (B) with a higher error rate could actually be better because it prevents undesirable images from appearing.\n",
    "\n",
    "### Solution:\n",
    "\n",
    "- Consider revising or changing the evaluation metric when the current one is no longer appropriate.\n",
    "\n",
    "- Suggest adding weights to examples in the development set to correctly reflect the severity of errors (e.g., errors with pornographic content have higher weights).\n",
    "\n",
    "- Optimize the evaluation metric to accurately reflect what the application needs to perform well on, rather than just relying on the number of errors.\n",
    "\n",
    "- Modify the development/dev set so that it more accurately reflects the data the algorithm needs to perform well on.\n",
    "\n",
    "- Change the assessment method or test set if they do not align with the actual goals of the product or service.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why human-level performance?\n",
    "\n",
    "- The **Bayes optimal error** represents a theoretical minimum error rate that cannot be surpassed by any classifier.\n",
    "\n",
    "- The error margin between human-level performance and the Bayes optimal error is usually small.\n",
    "\n",
    "- Humans excel in numerous tasks, and when machine learning algorithms perform worse than humans, one can:\n",
    "\n",
    "  - Obtain labeled data from human annotators.\n",
    "\n",
    "  - Analyze errors manually to understand the reasoning behind correct human predictions.\n",
    "  \n",
    "  - Analysis of bias and variance \n",
    "\n",
    "![Image](./image/WhyHuman.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoidable Bias\n",
    "\n",
    "- When working with machine learning algorithms, you aim for the algorithm to perform well on the training set but not to overfit compared to human performance.\n",
    "\n",
    "| Humans | Training error | Dev Error | Error |\n",
    "| ------ | -------------- | --------- | ----- |\n",
    "| 1%     | 8%             | 10%       | Bias |\n",
    "| 7.5%   | 8%             | 10%       | Variance |\n",
    "\n",
    "- The human-level error as a proxy (estimate) for Bayes optimal error. Bayes optimal error is always less (better), but human-level in most cases is not far from it.\n",
    "\n",
    "- Avoidable bias is the difference between the Bayes optimal error (estimated or actual) and the training error\n",
    "    - $Avoidable\\space Bias\\space =\\space Training\\space Error\\space -\\space Human\\space Error\\space (Bayes) $\n",
    "<br><br>\n",
    "- Variance is the gap between training error and development error.\n",
    "    - $Variance\\space =\\space Dev\\space error\\space -\\space Training\\space Error$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding Human-Level Performance\n",
    "\n",
    "\n",
    "- When choosing human-level performance, it has to be chosen in terms of what you want to achieve with the system.\n",
    "\n",
    "- You might have multiple human-level performances based on the human experience. Then you choose the human-level performance (proxy for Bayes error) that is more suitable for the system you're trying to build.\n",
    "\n",
    "- Improving deep learning algorithms is harder once you reach a human-level performance.\n",
    "\n",
    "- Summary of bias/variance with human-level performance:\n",
    "  1. human-level error (proxy for Bayes error)\n",
    "     \n",
    "     - Calculate: \n",
    "         $avoidable\\space bias\\space =\\space training\\space error\\space -\\space human-level\\space error$\n",
    "     \n",
    "     - If **avoidable bias** difference is the bigger, then it's a **bias** problem and you should use a strategy for bias resolving.\n",
    "  2. training error & dev error\n",
    "     \n",
    "     - Calculate: \n",
    "         $variance\\space =\\space dev\\space error\\space -\\space training\\space error$\n",
    "     \n",
    "     - If **variance** difference is bigger, then you should use a strategy for variance resolving.\n",
    "\n",
    "- So having an estimate of human-level performance gives you an estimate of Bayes error. And this allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm.\n",
    "- These techniques will tend to work well until you surpass human-level performance, whereupon you might no longer have a good estimate of Bayes error that still helps you make this decision really clearly.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
