{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Process\n",
    "\n",
    "- It's hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\n",
    "\n",
    "- One of the ways to tune is to sample a grid with $N$ hyperparameter settings and then try all settings combinations on your problem.\n",
    "\n",
    "- Try random values: don't use a grid.\n",
    "\n",
    "- You can use a **Coarse to fine sampling scheme**:\n",
    "  - When you find some hyperparameters values that give you a better performance.\n",
    "  - Zoom into a smaller region around these values and sample more densely within this space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using appropriate Scale\n",
    "\n",
    "- **Issue:**\n",
    "  - For finding the optimal learning rate alpha in the range $10^{-4}$ to 1, a uniform distribution leads to 90% of the samples between 0.1 and 1, which skews the search.\n",
    "\n",
    "- **Solution:**\n",
    "  - A distribution that allows equal exploration across all magnitudes of the range is necessary.\n",
    "  \n",
    "  - Logarithmic distribution for sampling ensures a balanced search across the potential range of values.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "- **Formulas:**\n",
    "  - Mean: $\\mu = \\frac{1}{m} \\sum_{i} z^{(i)} $\n",
    "  - Variance: $ \\sigma^2 = \\frac{1}{m} \\sum_{i} (z^{(i)} - \\mu)^2 $\n",
    "  - Normalization: $ z_{\\text{norm}}^{(i)} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $\n",
    "  - Scale and Shift: $ \\hat{z}^{(i)} = \\gamma z_{\\text{norm}}^{(i)} + \\beta $\n",
    "  \n",
    "- **Batch Norm in Neural Networks:**\n",
    "  - **Beta and Gamma:**\n",
    "    - Beta and gamma are parameters learned through backpropagation, optimizing the learning process.\n",
    "  - **Working with Mini Batches:**\n",
    "    - The bias parameter $ b $ is removed due to the normalization step $ Z = Z - \\text{mean} $.\n",
    "\n",
    "## Normalization Algorithm:\n",
    "\n",
    "- **Given:**\n",
    "  - $Z[1]$ array representing inputs \\( z(1), ..., z(m) \\), for \\( i \\) from 1 to \\( m \\) (for each input).\n",
    "\n",
    "- **Steps:**\n",
    "  - Compute the mean: $mean = 1/m * sum(Z[1])$.\n",
    "  - Compute the variance: $variance = 1/m * sum((Z[1] - mean)^2)$.\n",
    "  - Normalize: $Z_{norm[i]} = (Z[i] - mean) / np.sqrt(variance + epsilon)$ (add $epsilon$ for numerical stability if variance is 0), which forces inputs into a distribution with zero mean and variance of 1.\n",
    "  - Scale and Shift: $Z_{tilde[i]} = gamma * Z_{norm[i]} + beta$, adapting inputs to another distribution (with different mean and variance).\n",
    "\n",
    "## Why does Batch Normalization Work?\n",
    "\n",
    "- The reason is similar to why we normalize features input.\n",
    "\n",
    "- **Regularization**: \n",
    "  - Each mini batch is scaled by the mean/variance computed of that mini-batch.\n",
    "  \n",
    "  - This adds some noise to the values $Z[l]$ within that mini batch. So similar to dropout it adds some noise to each hidden layer's activations.\n",
    "\n",
    "## Problem at Test Time\n",
    "\n",
    "- **Issue:**\n",
    "  - Post-training, for effective prediction, normalization using the data's mean and variance is required.\n",
    "\n",
    "  - However, the mean and variance calculated per mini-batch differ, making them inapplicable directly to the test set.\n",
    "\n",
    "- **Solution: Exponentially Weighted Averages**.\n",
    "  \n",
    "  - Iteratively update the global mean and variance during each iteration to maintain consistent normalization for predictions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
