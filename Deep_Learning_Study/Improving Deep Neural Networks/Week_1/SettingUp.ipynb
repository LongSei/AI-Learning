{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Splitting\n",
    "- **Basic Split**: Training, dev, and test sets.\n",
    "\n",
    "- **Sourcing**: Importance of sourcing dev and test sets from the same data.\n",
    "\n",
    "- **Test Set Omission**: For non-critical estimations, focus on train/dev splits to avoid overfitting.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "- **Scenarios**: High variance (overfitting), high bias (underfitting)\n",
    "\n",
    "- **High Variance Solutions**: More data, regularization, neural network architecture search.\n",
    "\n",
    "- **High Bias Solutions**: Larger networks, longer training, architecture search.\n",
    "\n",
    "- **Error Evaluation**: Based on Bayes error for distinguishing error levels.\n",
    "\n",
    "\n",
    "|                         | High variance (overfit) | High bias (underfit) | High bias High variance | Low bias Low variance |\n",
    "|-------------------------|:-----------------------:|:--------------------:|:-----------------------:|:---------------------:|\n",
    "| **Training set error**  |           Low           |        High          |           High          |          Low          |\n",
    "| **Dev set error**       |           High          |        High          |         Much higher     |          Low          |\n",
    "\n",
    "\n",
    "![Image](./image/VarAndBias.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization \n",
    "\n",
    "## Overview\n",
    "- Regularization techniques are essential for reducing overfitting in machine learning models by adding penalty terms to the loss function, which helps in generalizing the model better to unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization\n",
    "\n",
    "- **L1 Regularization**: \n",
    "  $Cost = Loss + λ*(\\sum(|w|))$\n",
    "\n",
    "- **L2 Regularization**: $Cost = Loss + λ*(\\sum(w^2))$ <br>\n",
    "$\\implies Frobenius\\space Norm\\space for\\space Matrix\\space Calculation$: $||w^{[l]}||^{2} = \\sum_{i = 1}^{n^{[l]}}\\sum_{j = 1}^{n^{[l - 1]}}(w_{i, j}^{[l]})^{2}$<br>\n",
    "  - The rows $i$ of the matrix should be the number of neurons in the current layer  $n^{[l]}$;\n",
    "  - the columns $j$ of the weight matrix should equal the number of neurons in the previous layer $n^{[l - 1]}$\n",
    "\n",
    "**Intuition Behind Regularization:**\n",
    "  - High **lambda (λ)** values push weights (**W**) towards zero, simplifying the neural network.\n",
    "\n",
    "  - Regularization reduces their units impact, so making less prone to overfitting network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "  - Operates by setting a probability for each neuron in the network to be dropped or kept in a given training phase.\n",
    "\n",
    "- **Dropout Process:**\n",
    "  - During training, each neuron has a 50% chance of being kept or removed, simulating training on various network architectures.\n",
    "\n",
    "  - This probabilistic removal of neurons leads to a diminished network, forcing the model to adapt to different structures.\n",
    "  \n",
    "  - Ensures that the network does not become too dependent on any single neuron, promoting a more generalized learning.\n",
    "\n",
    "- **Benefits of Dropout:**\n",
    "  - Encourages the neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
    "\n",
    "  - Helps in preventing overfitting by ensuring that the model does not rely too heavily on any single path of neurons.\n",
    "\n",
    "  - At test time, the network benefits from the learning of an ensemble of varied sub-networks, improving generalization.\n",
    "\n",
    "- **Operational Insights:**\n",
    "  - For each training example, a different \"thinned\" network is used, with neurons dropped out randomly.\n",
    "\n",
    "  - This process simulates training multiple smaller networks and averaging their predictions, leading to a more robust model.\n",
    "\n",
    "- **Dropout Probility:**: \n",
    "  - For layers with a large number of units and large weight sizes, it is advisable to set keep_prob low to ensure there is no overfitting.\n",
    "\n",
    "\n",
    "![Image](./image/DropOut.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others\n",
    "\n",
    "### Early Stopping\n",
    "- Stops when there is a significant divergence between the loss functions of the development (dev) set and the training set.\n",
    "\n",
    "#### Limitations\n",
    "- Performs two tasks simultaneously, which can be conflicting: reducing the cost function (J) and avoiding overfitting, making the set of experiments (models, adjustments, etc.) complex.\n",
    "\n",
    "#### Alternative Approaches\n",
    "- L2 regularization can be used as an alternative by running many epochs, however, it requires experimenting with various lambda values for L2.\n",
    "\n",
    "![Image](./image/EarlyStopping.png)\n",
    "\n",
    "### Data Augmentation: \n",
    "- Generate more data by modify the origin data $\\implies$ Overfitting Solution\n",
    "\n",
    "![Image](./image/DataAug.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "  1. **Mean Normalization:**<br>\n",
    "      - $mean(X) = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}$\n",
    "      <br><br>\n",
    "      - $x_{norm} = \\frac{x - mean(x)}{std(x)}$\n",
    "      <br><br>\n",
    "  2. **Variance Normalization:**\n",
    "\n",
    "      - $std(x) = \\sqrt{\\frac{1}{n}\\sum_{i = 1}{n} (x_i - mean(x))^{2}}$\n",
    "      <br><br>\n",
    "      - $x_{norm} = \\frac{x}{std(x_{desired})}$\n",
    "\n",
    "$\\implies$: Help the distribution more equally\n",
    "\n",
    "![Image](./image/Distribution.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding\n",
    "\n",
    "- **Exploding Gradients:**\n",
    "  - If weight matrices are too big, the output grows too fast so it's hard to converge.\n",
    "\n",
    "- **Vanishing Gradients:**\n",
    "  - If weight matrices are too small, the update process will be slow down.\n",
    "\n",
    "- **Partial Solution:**\n",
    "  - Careful initialization of weights (Can initialize weights with small variance)\n",
    "    ```\n",
    "    W = np.random.randn(shape) * variance\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Approximation\n",
    "- **Approximation:** $\\frac{df(x)}{dx} = lim_{h->0}\\frac{f(x + h) - f(x - h)}{2*h}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Process\n",
    "\n",
    "- $Difference = \\frac{||grad\\space -\\space grad_{approximate}||_{2}}{\\space ||grad||_{2}\\space +\\space ||\\space grad_{approximate}\\space ||_{2}}$\n",
    "\n",
    "    <br>$\\implies$ If $Difference$ is larger than $\\epsilon$ (Ex: $10^{-7}$), hence the process have been wrong implement"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
