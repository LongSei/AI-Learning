{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **State (s)**: Represents the helicopter's position, orientation, and speed, detailing the current situation.\n",
    "\n",
    "- **Action (a)**: Decisions made to control the helicopter's movements based on its state.\n",
    "\n",
    "- **Reward**: The core of reinforcement learning, guiding the algorithm by rewarding positive outcomes (good helicopter) and penalizing negative ones (bad helicopter), similar to training a dog with feedback. This encourages the system to autonomously maximize positive outcomes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return \n",
    "- Discount Factor ($\\gamma$): A number slightly less than 1 $\\implies$ emphasizing the value of immediate gains in the decision-making process.\n",
    "- $Return = R_1 +\\gamma R_2+\\gamma ^2R_2+\\dots(until \\space terminal \\space state)$\n",
    "\n",
    "$\\therefore$ The Return will be different with different position (state)\n",
    "\n",
    "![Image](./image/Return.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "- **Policy ($\\pi$)**: A function that maps a given state ($s$) to an action ($a$) that the agent should take when in that state. The policy is denoted as $\\pi(s) = a$, indicating the action $a$ recommended when the system is in state $s$.\n",
    "\n",
    "## State-action value \n",
    "\n",
    "- **Definition $Q(s, a)$**:  it represents the return (total future rewards) expected for being in a state $s$ and taking an $a$, followed by following an optimal policy thereafter.\n",
    "\n",
    "- **Choose max Return**: A natural way is that we will compute the return for all $a$ (action). Then choose the best $Return$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "- Formula: $Q(s,a) = R(s) + \\gamma\\space m\\underset{a^{'}}ax Q(s',a')$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous State\n",
    "\n",
    "- In the real world, the state of entity can be continuous like in range of number. \n",
    "\n",
    "## Example\n",
    "\n",
    "#### 1. Autonomous Driving\n",
    "\n",
    "- **State Variables**: Position ($x, y$) on a map, velocity ($v$), orientation angle ($θ$), and acceleration ($a$).\n",
    "- **Description**: The continuous state space includes the car's precise location, speed, direction, and rate of speed change. The RL algorithm must decide on actions like steering angle adjustments, acceleration, or braking to navigate roads safely and efficiently.\n",
    "\n",
    "#### 2. Robotic Manipulation\n",
    "\n",
    "- **State Variables**: Joint angles ($θ1, θ2, ..., θn$), joint velocities ($θ̇1, θ̇2, ..., θ̇n$), and object positions ($x, y, z$).\n",
    "- **Description**: In robotic arm control, the state includes the angles and velocities of each joint for precision movements, as well as the position of objects the robot interacts with. The goal is often to manipulate objects or perform tasks with high dexterity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Build Up\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "\n",
    "- Our goal is to know which is the best action to take, then we need to know their Bellman Value $\\implies$ Our model is be trained to predict $Q(s,a)$ \n",
    "\n",
    "#### Paramters\n",
    "\n",
    "- $X: s \\space (State), a \\space (Action)$\n",
    "- $Y: Q(s,a) = R(s) + \\gamma\\space m\\underset{a^{'}}ax Q(s',a')$\n",
    "\n",
    "![Image](./image/DataCreation.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "![Image](./image/Algorithm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-greedy policy\n",
    "\n",
    "![Image](./image/epsilon.png)\n",
    "\n",
    "- There are some action will not be taken because of low $Return$. Despite it can be the better option. \n",
    "$\\implies$ To reduce that case, we will give it a small chance to appear (called epsilon ($\\epsilon$))\n",
    "\n",
    "## Trick: \n",
    "- We initialize it a high chance to randomly generate action for equal chance. Then after the model be better, gradually reduce it, because our model have been trained on that action (if that action is good, it will be kept and opposite)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch and soft-updates\n",
    "\n",
    "## Mini - Batch\n",
    "- **Definition**: Split the dataset into small set and train it several time $\\implies$ It will have increasing efficient\n",
    "\n",
    "![Image](./image/Mini-Batch.png)\n",
    "\n",
    "## Soft-Update\n",
    "- **Definition**: Not update the old one to new one directly. We weights them and sum it up to get the new one, this will have the new result also be affected by both new and old value\n",
    "\n",
    "![Image](./image/Soft-Update.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
