{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning \n",
    "\n",
    "![Image](./image/UnsupervisedLearning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "\n",
    "- **Unsupervised Learning**: Unlike supervised learning, where input features $x$ are paired with target outputs $y$ to learn from, unsupervised learning deals with data without explicit labels.\n",
    "\n",
    "- **Discovering Data Structure**: The goal is to uncover interesting structures within the data, such as grouping data points that are similar to each other into clusters.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process: \n",
    "\n",
    "1. **Initialization**: Randomly pick two points as the initial guesses for the cluster centers (centroids).\n",
    "\n",
    "![Image](./image/K-mean(1).png)\n",
    "\n",
    "2. **Assignment**: Assign each data point to the nearest cluster centroid. This step groups the data points based on which centroid they are closest to.\n",
    "\n",
    "![Image](./image/K-mean(2).png)\n",
    "\n",
    "3. **Update**: Update each cluster centroid to the average location of the data points assigned to it. This step recalculates the position of each centroid based on the current members of its cluster.\n",
    "\n",
    "![Image](./image/K-mean(3).png)\n",
    "\n",
    "4. **Repeat**: Alternate between the assignment and update steps until the centroids no longer change position, indicating the algorithm has converged.\n",
    "\n",
    "![Image](./image/K-mean(4).png)\n",
    "\n",
    "### Pseudocode: \n",
    "Initialize K cluster centroids randomly (mu_1, mu_2, ..., mu_k)\n",
    "\n",
    "```\n",
    "Repeat {\n",
    "  // Assignment step\n",
    "  For every point i in the dataset {\n",
    "    Assign the point to the closest centroid:\n",
    "    c_i = argmin_k ||x^(i) - mu_k||^2\n",
    "  }\n",
    "\n",
    "  // Update step\n",
    "  For each cluster centroid k {\n",
    "    Set mu_k to be the mean of points assigned to cluster k:\n",
    "    mu_k = mean(all points x^(i) where c_i = k)\n",
    "  }\n",
    "  \n",
    "  // Check for convergence\n",
    "  If no points change their cluster assignment and centroids don't move,\n",
    "  then stop the loop.\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "- The cost function J depends on:\n",
    "    - $ C_1, C_2, ..., C_m $: The cluster assignments for each point.\n",
    "    - $ \\mu_1, \\mu_2, ..., \\mu_k $: The locations of the cluster centroids.\n",
    "  - It is computed as: $ J = \\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)} - \\mu_{C_i}||^2 $.\n",
    "  - The goal is to minimize J by adjusting $ C_i $ and $ \\mu_k $.\n",
    "\n",
    "![Image](./image/CostFunction.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Initial Optimization\n",
    "\n",
    "![Image](./image/K-MeanInitialize.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Amount Clusters\n",
    "\n",
    "- **Ambiguity in Choosing K**:\n",
    "  - The 'correct' number of clusters (K) is not always clear-cut; different observers might see different cluster counts in the same dataset.\n",
    "\n",
    "- **Elbow Method**:\n",
    "  - Plot the cost function (distortion) for various values of K.\n",
    "  - Look for an 'elbow' where the cost function begins to decrease more slowly and choose K at that point.\n",
    "\n",
    "![Image](./image/ElbowMethod.png)\n",
    "\n",
    "- **Practical Approach**:\n",
    "  - Determine K based on the clusters' performance for a subsequent or downstream application.\n",
    "  - Evaluate how well K-means serves the intended purpose of clustering rather than just minimizing the cost function.\n",
    "\n",
    "![Image](./image/K-Value.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian (Normal) Distribution\n",
    "\n",
    "- **Mean (μ)**: The center of the curve, where the peak aligns.\n",
    "\n",
    "- **Standard Deviation $σ$**: Determines the width of the curve. Variance: $σ^{2} = \\frac{1}{m} * \\sum_{i = 1}^{m}(x^{(i)} - μ)^{2}$\n",
    "\n",
    "- **Probability Density Function $p(x)$**: $p(x) = \\frac{1}{\\sqrt{2πσ}} e^{\\frac{-(x - μ)^{2}}{(2σ²)}}$, showing how probable different values of $x$ are under the distribution.\n",
    "\n",
    "![Image](./image/NormalDistribution.png)\n",
    "  \n",
    "## Adjusting μ and σ:\n",
    "- When μ = 0 and σ = 1, it's the standard normal distribution.\n",
    "\n",
    "- Reducing σ narrows and heightens the curve, increasing σ widens and lowers it.\n",
    "\n",
    "- Changing μ shifts the center of the distribution left or right without altering its shape.\n",
    "\n",
    "## Anomaly Detection Application:\n",
    "- Given a dataset of normal examples, estimate good values for μ and σ².\n",
    "- μ is calculated as the average of the data points.\n",
    "- σ² is the average of the squared differences from the mean.\n",
    "- New examples with low p(x) (falling far from the center of the distribution) are flagged as anomalies.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "- **Purpose**: Anomaly detection algorithms identify unusual events or anomalies in unlabeled datasets of normal occurrences.\n",
    "\n",
    "- **Density Estimation**: The algorithm models the probability of feature values in the dataset, identifying high-probability (normal) and low-probability (anomalous) regions.\n",
    "\n",
    "- **Flagging Anomalies**: If the probability of a new data point's features is below a threshold ε (epsilon), it's flagged as an anomaly.\n",
    "\n",
    "![Image](./image/AnomalyDetection.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "![Image](./image/AnomalyDetectionAlgo.png)\n",
    "\n",
    "### Optimize\n",
    "- Split dataset\n",
    "    - **Training Set**: Large set of normal examples, anomalies are rare or assumed absent.\n",
    "    - **Cross Validation and Test Sets**: Mix of normal examples and a small number of known anomalies to evaluate and fine-tune the model.\n",
    "\n",
    "### Evaluate Metrics: \n",
    "- Use metrics like true positives, false positives, false negatives, and true negatives.\n",
    "Consider precision, recall, and F1 score instead of classification accuracy.\n",
    "\n",
    "### Choosing Feature\n",
    "- To improve anomaly detection, it's beneficial to ensure the features approximate a Gaussian distribution. \n",
    "\n",
    "- If a feature does not naturally fit this distribution, various transformations can be applied to make it more Gaussian-like. Common transformations include taking the $\\log_2{x}$, $\\sqrt{x}$, ... or any other math operation to get the bell-shape curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection vs. Supervised Learning\n",
    "\n",
    "| Criteria                        | Anomaly Detection                                                                 | Supervised Learning                                                               |\n",
    "|---------------------------------|-----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| **Number of Positive Examples** | Very few (0-20), with a larger set of negative examples.                           | Larger set of both positive and negative examples.                               |\n",
    "| **Learning from Examples**      | Learns parameters from negative examples. Positive examples used for evaluation.  | Learns from both positive and negative examples. Assumes future positives are similar to past ones. |\n",
    "| **Anomaly Diversity**           | Suited for detecting new, unseen types of anomalies.                              | Assumes future positive examples will be similar to those in the training set.   |\n",
    "| **Best Use Case**               | When future anomalies might be completely different from those in the training set. | When positive examples are consistent over time and future instances resemble past ones. |\n",
    "| **Applications**                | - New defect detection in manufacturing<br>- Fraud detection for new methods<br>- Security (evolving threats) | - Known defect detection in manufacturing<br>- Spam email classification<br>- Weather prediction<br>- Disease diagnosis |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
