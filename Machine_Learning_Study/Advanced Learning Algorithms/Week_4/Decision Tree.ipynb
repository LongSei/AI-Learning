{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "![Image](./image/DecisionTree.png)\n",
    "\n",
    "- A decision tree model is depicted as a tree-like structure where decision nodes (ovals or rectangles) represent points of feature evaluation that guide the classification path to leaf nodes (rectangles), which make the final prediction.\n",
    "\n",
    "- The process involves starting at the **root node** and making decisions based on feature values to traverse the tree until reaching a **leaf node** that predicts the subject's classification.\n",
    "\n",
    "- Based on the way choosing feature, there will be more tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "\n",
    "1. **Selecting the Current Node Feature**\n",
    "\n",
    "2. **Branching Based on Feature Values**\n",
    "\n",
    "3. **Repeat until the Final Node**\n",
    "    - 100% data have been label, or\n",
    "    - Exceed depth limit of tree, or\n",
    "    - Reach the purity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Decisions in Building Decision Trees:\n",
    "\n",
    "1. **Feature Selection for Splitting**:\n",
    "   - At each node, decide which feature to split on, aiming to maximize the purity of subclasses. The goal is to achieve subsets of examples that are as homogeneous as possible (all cats or all dogs).\n",
    "\n",
    "2. **Stopping Criteria**:\n",
    "   - Determine when to stop splitting, which could be based on achieving pure subsets, reaching a maximum tree depth, achieving minimal improvement in purity from a split, or having too few examples in a node to justify further splitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy:\n",
    "\n",
    "- Entropy measures the impurity (or uncertainty) in a dataset\n",
    "\n",
    "- **Formula**: Entropy is calculated as $H(p_1) = -p_1 \\log_2(p_1) - (1-p_1) \\log_2(1-p_1)$, where $p_1$ is the fraction of positive\n",
    "\n",
    "- The entropy value ranges from 0 (completely pure) to 1 (completely impure/mixed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Feature Based on Entropy\n",
    "\n",
    "![Image](./image/Entropy.png)\n",
    "\n",
    "- Choosing the feature which have the biggest information gained: $H(p_1^{root}) - (w^{left}H(p_1^{left}) + w^{right}H(p_1^{right}))$\n",
    "\n",
    "($w = \\frac{Amount\\space data\\space on\\space current\\space node}{Amount\\space data\\space on\\space root\\space node}$)\n",
    "\n",
    "![Image](./image/InformationGain.png)\n",
    "\n",
    "- We got entropy of root node: $p_1 = \\frac{5}{10} = 0.5 \\implies H(0.5) = 1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process with continuous feature\n",
    "\n",
    "- **Split it into small interval**\n",
    "\n",
    "![Image](./image/ProcessContinuous.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree vs. Neuron Network\n",
    "\n",
    "| Type            | Advantage |\n",
    "|-----------------|-----------|\n",
    "| Decision Tree   | - Only suitable for structured data. Unstructured data like images, audio, and text cannot be processed.<br>- Processes faster than neural networks.<br>- Small decision trees can be visually interpreted by humans. |\n",
    "| Neural Network  | - Efficiently handles all types of data.<br>- Capable of applying transfer learning.<br>- Facilitates easier integration when building systems with\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
