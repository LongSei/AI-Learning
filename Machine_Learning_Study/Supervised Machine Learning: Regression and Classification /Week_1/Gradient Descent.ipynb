{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview:**\n",
    "  - Gradient descent is a foundational algorithm in machine learning, not limited to linear regression but applicable to training advanced neural network models, among others. It provides a systematic approach for minimizing the cost function $J(w, b)$, optimizing parameters $w$ and $b$ to achieve the lowest possible cost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key Concepts:**\n",
    "  - **Gradient Descent Purpose:** Minimize the cost function $J(w, b)$ over model parameters $w$ and $b$.<br><br>\n",
    "  - **General Application:** Although illustrated with linear regression, gradient descent is a versatile method applicable to functions with multiple parameters ($w_1, w_2, \\ldots, w_n, b$) across various machine learning models.<br><br>\n",
    "  - **Initial Guess:** Starts with initial guesses for parameters, commonly set to 0 ($w = 0, b = 0$), and iteratively adjusts $w$ and $b$ to decrease $J(w, b)$.<br><br>\n",
    "  - **Process:** By iteratively taking steps in the direction that most steeply decreases $J$, gradient descent seeks to find the function's minimum, ideally the global minimum but possibly a local minimum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Process:**\n",
    "![ProImage](./image/GradientDescent.png)\n",
    "  - **Direction of Descent:** At each step, the algorithm evaluates the surrounding landscape to determine the direction that most steeply descends toward a minimum.<br><br>\n",
    "  - **Local Minima:** Depending on the starting point, gradient descent may lead to different local minima, illustrating the algorithm's sensitivity to initial conditions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing Gradient Descent**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Gradient Descent Formula:\n",
    "\n",
    "- **Update Rule:** On each iteration, parameters are updated as follows:\n",
    "  - $w := w - \\alpha \\frac{\\partial}{\\partial w}J(w,b)$\n",
    "  - $b := b - \\alpha \\frac{\\partial}{\\partial b}J(w,b)$\n",
    "  Where $\\alpha$ is the learning rate, a small positive number that controls the step size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematic behind the algorithm:\n",
    "- **Gradient:** \n",
    "    - In calculus, the gradient of a function represents the direction and rate of the fastest increase of the function. \n",
    "$->$ The fastest way to decrease the function is minus with Gradient <br><br>\n",
    "\n",
    "- **Learning Rate:** \n",
    "    - Because of the road to the optimize point will took much iterations to get, so change the large $\\alpha$ will make this process faster.\n",
    "    - But if the $\\alpha$ is too large, your algorithm can't get to the optimize point"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
