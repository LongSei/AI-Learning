{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "![Image](./image/SigmoidFunction.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Logistic Regression Model\n",
    "\n",
    "- **Model Foundation**: Begins with a linear combination of inputs \\(z = wx + b\\), which is then passed through the Sigmoid function.\n",
    "- **Model Equation**: $f(x) = g(wx + b) = \\frac{1}{1 + e^{-(wx + b)}}$, translating features into a probability between 0 and 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why sigmoid ?  $\\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "| Feature             | Description                                                                 |\n",
    "|---------------------|----------------------------------------------------------------------------|\n",
    "| **Output Range**    | Maps any input value to a range between 0 and 1, ideal for probability outputs. |\n",
    "| **Differentiability** | The function is differentiable at every point, crucial for optimization algorithms like gradient descent. |\n",
    "| **Non-linear Transformation** | Introduces non-linearity, enabling the model to learn complex patterns beyond linear relationships. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
