{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](./image/RegularizationWithCostFunction.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Modifying the Cost Function\n",
    "\n",
    "- The original cost function (e.g., mean squared error for linear regression) is modified by adding a term that penalizes large values of coefficients.\n",
    "$->$ $J(w, b) = \\frac{1}{2m} * (\\sum_{i=1}^{m}{(\\hat{y} - y^{(i)}) ^ {2}} + \\frac{\\lambda}{2m} * \\sum_{j=1}^{n}{w^{2}} + \\frac{\\lambda}{2m} * b^{2})$ (n: features, m: samples)\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "- **Regularization Term**: The added term $\\lambda \\times \\sum_{j=1}^{n} W_j^2$ encourages smaller coefficients, making the model simpler and less prone to overfitting.\n",
    "\n",
    "- **Lambda ($\\lambda$)**: The regularization parameter determines the trade-off between fitting the training data well and keeping the model coefficients small.\n",
    "\n",
    "- **Scaling by $\\frac{1}{2m}$**: Both the original cost term and the regularization term are scaled by $\\frac{1}{2m}$, where $m$ is the number of training examples. This scaling helps in choosing a suitable $\\lambda$ value that works well regardless of the training set size.\n",
    "\n",
    "#### Effects of Different Lambda Values\n",
    "\n",
    "- **$\\lambda = 0$**: No regularization is applied, leading to potential overfitting.\n",
    "\n",
    "- **Very Small $\\lambda$**: Doesn't have effect\n",
    "\n",
    "- **Very Large $\\lambda$**: Heavy penalty on coefficients, leading to underfitting as the model becomes too simple, potentially resulting in a horizontal line (constant function) that ignores the input features.\n",
    "\n",
    "- **Optimal $\\lambda$**: A value of $\\lambda$ that is neither too small nor too large will balance fitting the training data well and maintaining a simpler model, reducing the risk of overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
